import sys # å¯¼å…¥ sys
import subprocess
import os
from pathlib import Path
from tqdm import tqdm
import re
import torch
from faster_whisper import WhisperModel # ç¡®ä¿ Faster Whisper å·²å®‰è£…

class SubtitlesProcessor:
    def __init__(self, url: str, proxy: str = None):
        self.url = url
        self.proxy = proxy
        self.video_id = self._extract_video_id(url)
        self.audio_path = None
        # self.nltk_data_path å’Œç›¸å…³æ–¹æ³•å·²ç§»é™¤
        # jieba å¯¼å…¥å·²ç§»é™¤ï¼Œå› ä¸ºä¸å†è¿›è¡Œå¤æ‚åˆ†å¥

    def _proxy_arg(self) -> list[str]:
        return ["--proxy", self.proxy] if self.proxy else []

    def _extract_video_id(self, url: str) -> str:
        # ç®€åŒ–è§†é¢‘ ID æå–ï¼Œé’ˆå¯¹ YouTube URL
        match = re.search(r'(?:v=|\/)([a-zA-Z0-9_-]{11})(?:&|\?)?.*', url)
        if match:
            return match.group(1)
        return "unknown_video" # æ— æ³•æå– ID æ—¶çš„å¤‡ç”¨

    def download_audio(self, output_dir: str):
        """
        ä¸‹è½½è§†é¢‘éŸ³é¢‘ã€‚
        ä¼˜åŒ–ï¼šæ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡ä¸‹è½½ã€‚
        """
        print("ğŸµ Downloading audio...")
        os.makedirs(output_dir, exist_ok=True)
        
        # æ„å»ºå¯èƒ½çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨ %(title)s å’Œ %(ext)s
        # yt-dlp å¯èƒ½ä¼šä¸‹è½½ä¸º .webm, .m4a, .opus, .mp3 ç­‰
        # æˆ‘ä»¬éœ€è¦å…ˆå°è¯•æŸ¥æ‰¾å·²å­˜åœ¨çš„éŸ³é¢‘æ–‡ä»¶
        
        # æŸ¥æ‰¾å·²å­˜åœ¨çš„éŸ³é¢‘æ–‡ä»¶
        existing_audio_files = []
        for f_name in os.listdir(output_dir):
            # æŸ¥æ‰¾æ–‡ä»¶ååŒ…å« video_id ä¸”æ˜¯å¸¸è§éŸ³é¢‘æ ¼å¼çš„æ–‡ä»¶
            if self.video_id in f_name and (f_name.endswith(".m4a") or f_name.endswith(".webm") or f_name.endswith(".opus") or f_name.endswith(".mp3")):
                existing_audio_files.append(os.path.join(output_dir, f_name))
        
        if existing_audio_files:
            self.audio_path = existing_audio_files[0] # å–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„
            print(f"âœ… Audio file already exists: {self.audio_path}. Skipping download.")
            return

        # å¦‚æœéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™æ‰§è¡Œä¸‹è½½
        audio_output_path = os.path.join(output_dir, f"%(title)s[{self.video_id}].%(ext)s")
        command = [
            sys.executable, "-m", "yt_dlp", "-x", "-f", "bestaudio", self.url,
            "-o", audio_output_path
        ] + self._proxy_arg()
        
        try:
            print("å°è¯•ä¸‹è½½éŸ³é¢‘ï¼Œä½¿ç”¨ python -m yt_dlp -x -f bestaudio æ¨¡å¼...")
            subprocess.run(command, check=True) # ç§»é™¤ shell=Trueï¼Œå› ä¸ºç°åœ¨ç›´æ¥è°ƒç”¨ python
            
            # ä¸‹è½½å®Œæˆåï¼Œå†æ¬¡æŸ¥æ‰¾å®é™…ä¸‹è½½çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            # å› ä¸º yt-dlp ä¼šæ ¹æ® %(title)s ç”Ÿæˆå…·ä½“æ–‡ä»¶å
            newly_downloaded_audio_files = []
            for f_name in os.listdir(output_dir):
                if self.video_id in f_name and (f_name.endswith(".m4a") or f_name.endswith(".webm") or f_name.endswith(".opus") or f_name.endswith(".mp3")):
                    newly_downloaded_audio_files.append(os.path.join(output_dir, f_name))
            
            if newly_downloaded_audio_files:
                self.audio_path = newly_downloaded_audio_files[0] 
                print(f"âœ… Audio downloaded to: {self.audio_path}")
            else:
                print(f"âš ï¸ æ— æ³•åœ¨ '{output_dir}' ç›®å½•ä¸­æ‰¾åˆ°ä¸‹è½½çš„éŸ³é¢‘æ–‡ä»¶ï¼Œä½† yt-dlp å‘½ä»¤æˆåŠŸå®Œæˆã€‚")
                self.audio_path = None

        except subprocess.CalledProcessError as e:
            print(f"âŒ ä¸‹è½½éŸ³é¢‘å¤±è´¥: {e.stderr}")
            self.audio_path = None
        except Exception as e:
            print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            self.audio_path = None

    def Otranscribe_with_whisper(self, model_dir: str):
        if not self.audio_path or not Path(self.audio_path).exists():
            print("âŒ Audio file not found for transcription.")
            return []

        print(f"ğŸ™ï¸ Transcribing audio with Faster Whisper model: {model_dir}...")
        try:
            # ç¡®ä¿ torch å·²å¯¼å…¥ä¸”å¯ç”¨ï¼Œå¦‚æœå¯ç”¨åˆ™ä½¿ç”¨ cuda
            model = WhisperModel(model_dir, device="cuda" if torch.cuda.is_available() else "cpu", compute_type="float16")
            segments, info = model.transcribe(self.audio_path, beam_size=5)
            print(f"âœ… Detected language: {info.language} with probability {info.language_probability:.4f}")
            
            segments_list = list(segments) # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿å¤šæ¬¡è®¿é—®
            # print(f"DEBUG: Transcribed segments count: {len(segments_list)}") # è°ƒè¯•æ‰“å°å·²ç§»é™¤
            
            return segments_list
        except ImportError:
            print("âŒ PyTorch not found. Please install PyTorch for GPU support or ensure it's in your environment.")
            print("Falling back to CPU if possible, or try installing torch: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 (for CUDA 11.8)")
            try:
                model = WhisperModel(model_dir, device="cpu", compute_type="int8") # CPU æ¨¡å¼å¯ä»¥ä½¿ç”¨ int8 ä¼˜åŒ–
                segments, info = model.transcribe(self.audio_path, beam_size=5)
                print(f"âœ… Detected language: {info.language} with probability {info.language_probability:.4f}")
                
                segments_list = list(segments)
                # print(f"DEBUG: Transcribed segments count (CPU fallback): {len(segments_list)}") # è°ƒè¯•æ‰“å°å·²ç§»é™¤
                
                return segments_list
            except Exception as e:
                print(f"âŒ Error during CPU transcription fallback: {e}")
                return []
        except Exception as e:
            print(f"âŒ Error during transcription: {e}")
            return []


    def OOtranscribe_with_whisper(self, model_dir: str):
        if not self.audio_path or not Path(self.audio_path).exists():
            print("âŒ Audio file not found for transcription.")
            return []

        print(f"ğŸ™ï¸ Transcribing audio with Faster Whisper model: {model_dir}...")
        try:
            model = WhisperModel(model_dir, device="cuda" if torch.cuda.is_available() else "cpu", compute_type="float16")
            
            # ä½¿ç”¨ tqdm æ¥æ˜¾ç¤ºè¿›åº¦æ¡
            # Faster Whisper çš„ transcribe æ–¹æ³•è¿”å›ä¸€ä¸ªè¿­ä»£å™¨
            # info å¯¹è±¡åŒ…å« duration_msï¼Œå¯ä»¥ç”¨æ¥ä¼°ç®—æ€»è¿›åº¦
            # ä¹Ÿå¯ä»¥ç›´æ¥è¿­ä»£ segmentsï¼Œtqdm ä¼šè‡ªåŠ¨è®¡ç®—è¿­ä»£æ¬¡æ•°
            
            # ç¬¬ä¸€æ¬¡è°ƒç”¨ transcribe æ—¶è·å– segments å’Œ info
            segments_generator, info = model.transcribe(self.audio_path, beam_size=5)
            print(f"âœ… Detected language: {info.language} with probability {info.language_probability:.4f}")
            
            segments_list = []
            # è·å–éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºè¿›åº¦æ¡
            total_audio_duration_ms = info.duration * 1000 
            
            # ä½¿ç”¨ tqdm åŒ…è£… segments è¿­ä»£å™¨ï¼Œå¹¶æ ¹æ®éŸ³é¢‘æ—¶é•¿æ˜¾ç¤ºè¿›åº¦
            # desc: è¿›åº¦æ¡æè¿°
            # total: è¿­ä»£çš„æ€»é‡ï¼Œè¿™é‡Œç”¨éŸ³é¢‘æ€»æ—¶é•¿
            # unit: å•ä½
            # unit_scale: å•ä½çš„ç¼©æ”¾æ¯”ä¾‹ (ms -> s)
            # bar_format: è‡ªå®šä¹‰è¿›åº¦æ¡æ ¼å¼ï¼Œæ˜¾ç¤ºå·²è½¬å½•æ—¶é•¿
            
            # è®°å½•èµ·å§‹æ—¶é—´ï¼Œç”¨äºè®¡ç®—å·²è½¬å½•æ—¶é•¿
            start_time_segment = 0.0

            # æ”¶é›† segments å¹¶åœ¨ tqdm ä¸­æ˜¾ç¤ºè¿›åº¦
            for segment in tqdm(segments_generator, 
                                desc="ğŸ”Š Transcribing Audio", 
                                unit="s", unit_scale=True,
                                total=total_audio_duration_ms / 1000, # total in seconds
                                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]"):
                
                segments_list.append(segment)
                # æ›´æ–°è¿›åº¦æ¡çš„å½“å‰å€¼ï¼Œä½¿ç”¨å½“å‰ segment çš„ç»“æŸæ—¶é—´
                tqdm.write_text(tqdm._instances[0], f"Processed: {self._format_timestamp(segment.end)}", end='\r') # æ˜¾ç¤ºå½“å‰è½¬å½•åˆ°å“ªä¸ªæ—¶é—´ç‚¹
                
            return segments_list
            
        except ImportError:
            print("âŒ PyTorch not found. Please install PyTorch for GPU support or ensure it's in your environment.")
            print("Falling back to CPU if possible, or try installing torch: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 (for CUDA 11.8)")
            try:
                model = WhisperModel(model_dir, device="cpu", compute_type="int8")
                
                segments_generator, info = model.transcribe(self.audio_path, beam_size=5)
                print(f"âœ… Detected language: {info.language} with probability {info.language_probability:.4f}")
                
                segments_list = []
                total_audio_duration_ms = info.duration * 1000
                
                for segment in tqdm(segments_generator, 
                                    desc="ğŸ”Š Transcribing Audio (CPU)", 
                                    unit="s", unit_scale=True,
                                    total=total_audio_duration_ms / 1000,
                                    bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]"):
                    segments_list.append(segment)
                    tqdm.write_text(tqdm._instances[0], f"Processed: {self._format_timestamp(segment.end)}", end='\r')
                    
                return segments_list
            except Exception as e:
                print(f"âŒ Error during CPU transcription fallback: {e}")
                return []
        except Exception as e:
            print(f"âŒ Error during transcription: {e}")
            return []


    def transcribe_with_whisper(self, model_dir: str):
        if not self.audio_path or not Path(self.audio_path).exists():
            print("âŒ Audio file not found for transcription.")
            return []

        print(f"ğŸ™ï¸ Transcribing audio with Faster Whisper model: {model_dir}...")
        try:
            model = WhisperModel(model_dir, device="cuda" if torch.cuda.is_available() else "cpu", compute_type="float16")
            
            segments_generator, info = model.transcribe(self.audio_path, beam_size=5)
            print(f"âœ… Detected language: {info.language} with probability {info.language_probability:.4f}")
            
            segments_list = []
            
            # ç®€åŒ–è¿›åº¦æ˜¾ç¤ºï¼Œåªæ‰“å°å·²å¤„ç†çš„æ—¶é—´
            total_duration_seconds = info.duration # è·å–éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆç§’ï¼‰

            print("ğŸ”Š Transcription Progress:")
            
            # ä½¿ç”¨ tqdm ä½œä¸ºç®€å•çš„è¿­ä»£å™¨åŒ…è£…ï¼Œä¸å†ä¾èµ–å…¶å¤æ‚çš„ bar_format å’Œ write_text
            # ä»…é€šè¿‡è¿­ä»£ segment æ¥æ›´æ–°è¿›åº¦
            for i, segment in enumerate(segments_generator):
                segments_list.append(segment)
                
                # è®¡ç®—å·²å¤„ç†çš„ç™¾åˆ†æ¯”å’Œæ—¶é—´
                progress_percentage = (segment.end / total_duration_seconds) * 100 if total_duration_seconds > 0 else 0
                
                # æ‰“å°å·²è½¬å½•çš„æ—¶é•¿ï¼Œä½¿ç”¨ \r å›åˆ°è¡Œé¦–ï¼Œå®ç°åŠ¨æ€æ›´æ–°
                # flush=True ç¡®ä¿ç«‹å³æ‰“å°
                print(f"  Processed: {self._format_timestamp(segment.end)} / {self._format_timestamp(total_duration_seconds)} ({progress_percentage:.1f}%)", end='\r', flush=True)
            
            # è½¬å½•å®Œæˆåï¼Œæ‰“å°ä¸€ä¸ªç©ºè¡Œæˆ–å®Œæˆä¿¡æ¯ï¼Œæ¸…é™¤ä¸Šä¸€è¡Œçš„ \r æ•ˆæœ
            print("\nâœ… Transcription Complete!")
            
            return segments_list
            
        except ImportError:
            print("âŒ PyTorch not found. Please install PyTorch for GPU support or ensure it's in your environment.")
            print("Falling back to CPU if possible, or try installing torch: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 (for CUDA 11.8)")
            try:
                model = WhisperModel(model_dir, device="cpu", compute_type="int8")
                
                segments_generator, info = model.transcribe(self.audio_path, beam_size=5)
                print(f"âœ… Detected language: {info.language} with probability {info.language_probability:.4f}")
                
                segments_list = []
                total_duration_seconds = info.duration

                print("ğŸ”Š Transcription Progress (CPU Fallback):")
                for i, segment in enumerate(segments_generator):
                    segments_list.append(segment)
                    
                    progress_percentage = (segment.end / total_duration_seconds) * 100 if total_duration_seconds > 0 else 0
                    print(f"  Processed: {self._format_timestamp(segment.end)} / {self._format_timestamp(total_duration_seconds)} ({progress_percentage:.1f}%)", end='\r', flush=True)
                
                print("\nâœ… Transcription Complete (CPU Fallback)!")
                
                return segments_list
            except Exception as e:
                print(f"âŒ Error during CPU transcription fallback: {e}")
                return []
        except Exception as e:
            print(f"âŒ Error during transcription: {e}")
            return []

    def export_srt(self, segments: list, output_dir: str, filename: str) -> str:
        """
        å¯¼å‡ºä¸º SRT å­—å¹•æ–‡ä»¶ã€‚æ­¤æ–¹æ³•ä¿ç•™ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨æˆ–ä¸ä½¿ç”¨ã€‚
        """
        print("ğŸ“¤ Exporting subtitles to SRT file...")
        srt_path = os.path.join(output_dir, f"{filename}.srt")
        with open(srt_path, "w", encoding="utf-8") as f:
            for i, segment in enumerate(tqdm(segments, desc="ğŸ“„ Exporting SRT")):
                start_time = self._format_timestamp(segment.start)
                end_time = self._format_timestamp(segment.end)
                f.write(f"{i + 1}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment.text.strip()}\n\n")
        print(f"âœ… SRT exported to: {srt_path}")
        return srt_path

    def _format_timestamp(self, seconds: float) -> str:
        milliseconds = int((seconds * 1000) % 1000)
        seconds_int = int(seconds)
        minutes = seconds_int // 60
        hours = minutes // 60
        return f"{hours:02}:{minutes % 60:02}:{seconds_int % 60:02},{milliseconds:03}"

    def export_line_txt(self, srt_file_path: str, output_dir: str):
        """
        å°† SRT æ–‡ä»¶è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªå­—å¹•æ¡ç›®ï¼Œå»é™¤æ—¶é—´è½´å’Œåºå·ã€‚
        """
        print(f"ğŸ“– Starting to process SRT file for raw text export: {srt_file_path}")

        manuscript_lines = []
        try:
            with open(srt_file_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    # ç¡®ä¿åªæå–å®é™…çš„æ–‡æœ¬è¡Œï¼Œè·³è¿‡æ•°å­—å’Œæ—¶é—´æˆ³è¡Œ
                    if not line.isdigit() and "-->" not in line and line:
                        manuscript_lines.append(line)
        except FileNotFoundError:
            print(f"âŒ Error: SRT file not found at {srt_file_path}")
            return
        except Exception as e:
            print(f"âŒ Error reading SRT file {srt_file_path}: {e}")
            return

        if not manuscript_lines:
            print("âš ï¸ No text extracted from SRT file. Aborting raw text export.")
            return
        
        print("âš™ï¸ å°† SRT è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œå»é™¤æ—¶é—´è½´å¹¶æŒ‰è¡Œè¾“å‡º...")
        original_filename_base = Path(srt_file_path).stem
        manuscript_path = os.path.join(output_dir, f"{original_filename_base}_line.txt") # å‘½åä¸º _line.txt
        
        print(f"ğŸ“ Saving line-by-line text to: {manuscript_path}")
        with open(manuscript_path, "w", encoding="utf-8") as f:
            for line in manuscript_lines:
                f.write(line + "\n") # æ¯è¡Œä¸€ä¸ª SRT æ¡ç›®çš„æ–‡æœ¬
        print("âœ… Line-by-line text saved.")
